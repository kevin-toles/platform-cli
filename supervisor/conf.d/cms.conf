; =============================================================================
; context-management-service (CMS) Configuration
; =============================================================================
; WBS-LOG1.7: context-management-service.conf definition
; Port: 8086
; Service: Context Management Service with dual-routing
; =============================================================================

[program:context-management-service]
command=/Users/kevintoles/POC/context-management-service/.venv/bin/python -m uvicorn src.main:app --host 0.0.0.0 --port 8086
directory=/Users/kevintoles/POC/context-management-service
user=kevintoles
autostart=true
autorestart=true
startsecs=10
stopwaitsecs=30
startretries=3
exitcodes=0

; Logging (AC-LOG1.2)
stdout_logfile=/Users/kevintoles/Library/Logs/ai-platform/cms/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=5
stderr_logfile=/Users/kevintoles/Library/Logs/ai-platform/cms/stderr.log
stderr_logfile_maxbytes=10MB
stderr_logfile_backups=5
redirect_stderr=false

; Environment variables (AC-LOG0.4)
; CMS routes to both inference-service (local) and llm-gateway (cloud)
environment=
    CMS_LOG_LEVEL="INFO",
    CMS_INFERENCE_SERVICE_URL="http://localhost:8085",
    CMS_LLM_GATEWAY_URL="http://localhost:8080",
    REDIS_URL="redis://localhost:6379",
    PYTHONUNBUFFERED="1",
    PYTHONPATH="/Users/kevintoles/POC/context-management-service"

; Process priority (lower = start earlier)
; CMS should start last as it depends on inference-service and llm-gateway
priority=500
