; =============================================================================
; inference-service Configuration
; =============================================================================
; WBS-LOG1.3: inference-service.conf service definition
; Port: 8085
; Service: Local LLM inference with 16+ models
; =============================================================================

[program:inference-service]
command=/Users/kevintoles/POC/inference-service/.venv/bin/python -m uvicorn src.main:app --host 0.0.0.0 --port 8085
directory=/Users/kevintoles/POC/inference-service
user=kevintoles
autostart=true
autorestart=true
startsecs=30
stopwaitsecs=60
startretries=3
exitcodes=0

; Logging (AC-LOG1.2)
stdout_logfile=/Users/kevintoles/Library/Logs/ai-platform/inference-service/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=5
stderr_logfile=/Users/kevintoles/Library/Logs/ai-platform/inference-service/stderr.log
stderr_logfile_maxbytes=10MB
stderr_logfile_backups=5
redirect_stderr=false

; Environment variables (AC-LOG0.4)
; Note: GPU layers = -1 means use all available GPU layers (Metal on macOS)
environment=
    INFERENCE_SERVICE_LOG_LEVEL="INFO",
    INFERENCE_GPU_LAYERS="-1",
    INFERENCE_MODELS_DIR="/Users/kevintoles/POC/ai-models/models",
    INFERENCE_CONFIG_DIR="/Users/kevintoles/POC/inference-service/config",
    PYTHONUNBUFFERED="1",
    PYTHONPATH="/Users/kevintoles/POC/inference-service"

; Process priority (lower = start earlier)
; inference-service should start early as other services depend on it
priority=100
