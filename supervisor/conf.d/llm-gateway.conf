; =============================================================================
; llm-gateway Configuration
; =============================================================================
; WBS-LOG1.4: llm-gateway.conf service definition
; Port: 8080
; Service: Cloud LLM routing, auth, rate limiting
; =============================================================================

[program:llm-gateway]
command=/Users/kevintoles/POC/llm-gateway/.venv/bin/python -m uvicorn src.main:app --host 0.0.0.0 --port 8080
directory=/Users/kevintoles/POC/llm-gateway
user=kevintoles
autostart=true
autorestart=true
startsecs=10
stopwaitsecs=30
startretries=3
exitcodes=0

; Logging (AC-LOG1.2)
stdout_logfile=/Users/kevintoles/Library/Logs/ai-platform/llm-gateway/stdout.log
stdout_logfile_maxbytes=10MB
stdout_logfile_backups=5
stderr_logfile=/Users/kevintoles/Library/Logs/ai-platform/llm-gateway/stderr.log
stderr_logfile_maxbytes=10MB
stderr_logfile_backups=5
redirect_stderr=false

; Environment variables (AC-LOG0.4)
environment=
    LLM_GATEWAY_LOG_LEVEL="INFO",
    PYTHONUNBUFFERED="1",
    PYTHONPATH="/Users/kevintoles/POC/llm-gateway"

; Process priority (lower = start earlier)
; llm-gateway starts after inference-service
priority=200
